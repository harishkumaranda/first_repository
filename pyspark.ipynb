{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e08a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c75e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"testt\").master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d53ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"abc\",\"pqr\"),(\"abc\",\"pqr\"),(\"gty\",None)]\n",
    "schema = [\"name\",\"value\"]\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e514113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.schema.names\n",
    "print(df)\n",
    "df1 = ','.join(df)\n",
    "# df4 = df1.map(lambda x:x,1)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691dd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"abc\",\"pqr\"),(\"abc\",\"pqr\")]\n",
    "schema = [\"name\",\"value\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data = data, schema = schema)\n",
    "# df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aaa185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %run logging.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").option(\"skipFirst\", \"4\").load(\"employees.csv\")\n",
    "# print(df2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc32fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").option(\"endingPosition\", \"-10\").load(\"employees.csv\")\n",
    "print(df4.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82930018",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last = int(df2.count() * 0.5)\n",
    "dff_last = df2.limit(df_last).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_half = df2.sample(fraction = 0.2)\n",
    "print(df_half.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hhalf = df2.limit(int(df2.count()/2))\n",
    "print(df_hhalf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.schema.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7475bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = [StructField(i.name ,StringType(),True) for i in df2.schema.fields]\n",
    "new_schema = StructType(schema)\n",
    "print(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d76794",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(set(df2.columns) - set(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29daf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from functions import reduce\n",
    "new_columns =[\"agv\",\"hjk\"]\n",
    "df3 = reduce(lambda new_df, x : new_df.withColumn(x,lit(None)), new_colums, df2)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25104451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# cls = df2.columns\n",
    "# print(cls)\n",
    "cools = df2.columns+[\"abc\",\"pqr\"]\n",
    "print(cools)\n",
    "for i in cools:\n",
    "\n",
    "    if i not in df2.columns:\n",
    "        df2 = df2.withColumn(i,lit(None))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"EMPLOYEE_ID\",\"JOB_ID\"]\n",
    "for i in df2.columns:\n",
    "    if i in cols:\n",
    "        df2 = df2.withColumnRenamed(i,i+\"_1\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df3 =df2.select([count(when(col(i).isNull(),i)).alias(i) for i in df2.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f956b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "df2 = spark.read.csv(\"employees.csv\",header = \"true\",inferSchema = \"true\")\n",
    "df6 = df2.dropDuplicates([\"JOB_ID\"])\n",
    "df7 =df6.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count()\n",
    "# df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24389af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df7.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bad9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df7.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2479511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df9.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "df11 =df9.withColumn(\"partitionId\",spark_partition_id()).orderBy(asc(\"partitionid\")).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstdf2 = df2.columns\n",
    "print(lstdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da64aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import row_number,rank\n",
    "import datetime\n",
    "df2.printSchema()\n",
    "# w =Window.partitionBy(\"DEPARTMENT_ID\").orderBy(\"SALARY\")\n",
    "# df4 = df2.withColumn(\"HIRE_DATE\",to_date(col(\"HIRE_DATE\").cast(DateType())))\n",
    "# df4.printSchema()\n",
    "# .withColumn(\"date\",to_date(current_date(),\"dd-MMM-yy\")).withColumn(\"chk\",lit(None))\n",
    "\n",
    "df3 = df2.withColumn(\"date_diff\",datediff(to_date(current_date(),\"dd-MMM-yy\"),col(\"HIRE_DATE\").cast(DateType())))\n",
    "\n",
    "df3.printSchema()\n",
    "\n",
    "# df6 = df4.select([count(when(col(i)),i).alias(i) for i in df4.columns]).show()\n",
    "# lstdf4 = df4.columns\n",
    "# print(lstdf4)\n",
    "# df6 =df4.select([count(when(col(c).isNotNull(),c)).alias(c) for c in df4.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allcol = lstdf2+lstdf4\n",
    "\n",
    "# for i in allcol:\n",
    "#     if i not in lstdf4:\n",
    "#         df4 = df4.withColumn(i,lit(None))\n",
    "#     if i not in lstdf2:\n",
    "#         df2 = df2.withColumn(i,lit(None))\n",
    "# df4.show()\n",
    "# df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ae8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = spark.sql(\"select * from chk a join chk b on a.EMPLOYEE_ID = b.MANAGER_ID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addmissingcolumn(df1,df2):\n",
    "    from pyspark.sql.functions import lit\n",
    "    allcolumns = df1.columns+df2.columns\n",
    "    print(\"allcoumns:\",allcolumns)\n",
    "    for i in allcolumns:\n",
    "        if i not in df1.columns:\n",
    "            df1 = df1.withColumn(i,lit(None))\n",
    "        if i not in df2.columns:\n",
    "            df2.df2.withColumn(i,lit(None))\n",
    "    return df1,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "addmissingcolumn(df2,df4)\n",
    "df2.show()\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ee294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c1fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.write.format('delta').mode('overwrite').save(\"chk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cf036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.sql.extensions\":\"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "        \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e82b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
